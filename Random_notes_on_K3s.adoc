# K3s

* Server install command that sets the permissions correctly for the KUBECONFIG file: `curl -sfL https://get.k3s.io | K3S_KUBECONFIG_MODE=644 sh -s -`
* Agent install command (K3S_URL sets the node to worker mode): `curl -sfL https://get.k3s.io | K3S_URL=https://<FQDN of server>:6443 K3S_TOKEN=<token> sh -`
** The token is found on the Agent node in `/var/lib/rancher/k3s/server/node-token`
** Likely need to provide LB address for the FQDN of the server for HA
*** Though, not if all external to API server access will go through Rancher server

## Clean way of deploying HA K3s with embedded etcd:

* Deploy K3s with --cluster-init to enable embedded etcd:

----
K3s_VERSION=""      #I.e. "v1.20.4+k3s1"
curl -sfL https://get.k3s.io | INSTALL_K3S_VERSION=${K3s_VERSION} INSTALL_K3S_EXEC='server --cluster-init --write-kubeconfig-mode=644' sh -s -
----

* Install additional servers:

** Set variables for next command:

----
FIRST_SERVER_IP=""      # Private IP preferred, if available
NODE_TOKEN=""           # From the /var/lib/rancher/k3s/server/node-token file on the first server
K3s_VERSION=""          # Match the version of the first server
----

** Install command:

----
curl -sfL https://get.k3s.io | INSTALL_K3S_VERSION=${K3s_VERSION} K3S_URL=https://${FIRST_SERVER_IP}:6443 K3S_TOKEN=${NODE_TOKEN} K3S_KUBECONFIG_MODE="644" INSTALL_K3S_EXEC='server' sh -
----

* Install additional agents:

** Set variables for next command:

----
FIRST_SERVER_IP=""      # Private IP preferred, if available
NODE_TOKEN=""           # From the /var/lib/rancher/k3s/server/node-token file on the first server
K3s_VERSION=""          # Match the first of the first server
----

** Install command:

----
curl -sfL https://get.k3s.io | INSTALL_K3S_VERSION=${K3s_VERSION} K3S_URL=https://${FIRST_SERVER_IP}:6443 K3S_TOKEN=${NODE_TOKEN} K3S_KUBECONFIG_MODE="644" sh -
----

## An expanded scriptable method of creating an HA K3s cluster: 

* Deploy K3s with --cluster-init to enable embedded etcd:

----
wget https://github.com/k3s-io/k3s/releases/download/v1.20.4%2Bk3s1/k3s
sudo mv k3s /usr/local/bin/
sudo chmod 755 /usr/local/bin/k3s
curl -sfL https://get.k3s.io > install.sh
chmod 755 install.sh
INSTALL_K3S_SKIP_DOWNLOAD=true INSTALL_K3S_EXEC='server --cluster-init --write-kubeconfig-mode=644'  ./install.sh
----

* Add another server to the cluster

** Set variables for next command:

----
FIRST_SERVER_IP=""      # Private IP preferred, if available
NODE_TOKEN=""           # From the /var/lib/rancher/k3s/server/node-token file on the first server
K3s_VERSION=""          # Match the version of the first server
----

** Install command:

----
wget https://github.com/k3s-io/k3s/releases/download/v1.20.4%2Bk3s1/k3s
sudo mv k3s /usr/local/bin/
sudo chmod 755 /usr/local/bin/k3s
curl -sfL https://get.k3s.io > install.sh
chmod 755 install.sh
FIRST_SERVER_IP=""
sudo K3S_TOKEN=${NODE_TOKEN} INSTALL_K3S_SKIP_DOWNLOAD=true INSTALL_K3S_EXEC='server --server https://${FIRST_SERVER_IP}:6443 --write-kubeconfig-mode=644'  ./install.sh
----

## Creating K3s cluster with Longhorn (run as root):

* Set variables for next command:

----
K3s_VERSION=""	#I.e. "v1.20.4+k3s1"
CLUSTER=""	#set to --cluster-init for embedded etcd, otherwise leave blank
----

** Install command that disables local-path StorageClass:

----
curl -sfL https://get.k3s.io | INSTALL_K3S_VERSION=${K3s_VERSION} INSTALL_K3S_EXEC="server ${CLUSTER} --write-kubeconfig-mode=644 --disable local-storage" sh -s -
----

** Make sure K3s is completely deployed: `kubectl get pods -A`

** Install Longhorn:

----
kubectl apply -f https://raw.githubusercontent.com/longhorn/longhorn/master/deploy/longhorn.yaml
----

# Installing with an external database:

* Set variables for next command:

----
MDB_ROOT_PASSWORD=
MDB_K3S_USERNAME=
MDB_K3S_USER_PASSWORD=
MDB_VIP=
# Can be host public IP, LB or Keepalived VIP
K3S_DATABASE=
----


* Create containerized MariaDB with Podman:

----
sudo mkdir -p /opt/k3s-mariadb
sudo rm -rf /opt/k3s-mariadb/*		#Clean up in the case of re-creating DB pod

# sudo podman pod create --name k3s-mariadb-pod --publish 8080:80 --publish 3306:3306

# sudo podman run --name k3s-mariadb --pod k3s-mariadb-pod -v /opt/k3s-mariadb:/var/lib/mysql:Z -e  MYSQL_ROOT_PASSWORD="${MDB_ROOT_PASSWORD}" -e MYSQL_USER="${MDB_K3S_USERNAME}" -e MYSQL_PASSWORD="${MDB_K3S_USER_PASSWORD}" -d docker.io/library/mariadb

sudo podman run --name k3s-mariadb --publish 8080:80 --publish 3306:3306 -v /opt/k3s-mariadb:/var/lib/mysql:Z -e  MYSQL_ROOT_PASSWORD="${MDB_ROOT_PASSWORD}" -e MYSQL_USER="${MDB_K3S_USERNAME}" -e MYSQL_PASSWORD="${MDB_K3S_USER_PASSWORD}" -d docker.io/library/mariadb
----

* Create k3s database and user (Don't know how to pass variables into MariaDB yet):

----
sudo zypper in mariadb-client

#mariadb -h ${MDB_VIP} -u root -p${MDB_ROOT_PASSWORD}

mariadb -h ${MDB_VIP} -u root -p${MDB_ROOT_PASSWORD} -e "create database ${K3S_DATABASE};"

mariadb -h ${MDB_VIP} -u root -p${MDB_ROOT_PASSWORD} -e "grant all privileges on ${K3S_DATABASE}.* to 'k3s'@'%'  identified by 'k3sPassw0rd';"

mariadb -h ${MDB_VIP} -u root -p${MDB_ROOT_PASSWORD} -e "flush privileges;"
----

** NOTE: To re-install K3s, must reconnect to the DB and run:

----
mariadb -h ${MDB_VIP} -u root -p${MDB_ROOT_PASSWORD} -e "drop database ${K3S_DATABASE};"
mariadb -h ${MDB_VIP} -u root -p${MDB_ROOT_PASSWORD} -e "create database ${K3S_DATABASE};"
----

* Set variables for next command:

----
K3s_VERSION=	  	# E.g. "v1.20.4+k3s1", otherwise leave blank for latest
LOCAL_PATH_SC=		# Set to "--disable local-storage" to disable the local-path StorageClass, otherwise leave blank
MDB_K3S_USERNAME=	# If not already set, e.g. k3s, the same as the user created in MariaDB
MDB_K3S_PASSWORD=	# If not already set, e.g. k3sPass0rd, the same as the password created in MariaDB

MDB_VIP=		# If not already set, IP address or resolvable FQDN or hostname 
K3S_DATABASE=
----

* IMPORTANT: The install command below has given me trouble with all of the variable substitutions. If it fails, replace all variables with their assigned values
* Install command:

----
curl -sfL https://get.k3s.io | K3S_DATASTORE_ENDPOINT="mysql://${MDB_K3S_USERNAME}:${MDB_K3S_PASSWORD}@tcp\(${MDB_VIP}:3306\)/${K3S_DATABASE}" INSTALL_K3S_VERSION=${K3s_VERSION} INSTALL_K3S_EXEC="server --kube-controller-manager-arg=pod-eviction-timeout=80s --write-kubeconfig-mode=644 ${LOCAL_PATH_SC}" sh -s -
----

** Note: --kube-controller-manager-arg=pod-eviction-timeout=80s allows for a two minutes Kubelet failure detection

* Add another server to make the cluster HA:

** Set variables for next command:

----
FIRST_SERVER_IP=""      # LB or Keepalived IP preferred, if available
NODE_TOKEN=""           # From the /var/lib/rancher/k3s/server/node-token file on the first server
K3s_VERSION=""          # Match the version of the first server
LOCAL_PATH_SC=""	# Match the setting of the first server, e.g. "--disable local-storage" or blank
----

** Install command:

----
curl -sfL https://get.k3s.io | K3S_DATASTORE_ENDPOINT="mysql://${MDB_K3S_USERNAME}:${MDB_K3S_PASSWORD}@tcp(${MDB_VIP}:3306)/k3s" INSTALL_K3S_VERSION=${K3s_VERSION} K3S_TOKEN=${NODE_TOKEN} K3S_KUBECONFIG_MODE="644" INSTALL_K3S_EXEC="server --write-kubeconfig-mode=644 ${LOCAL_PATH_SC}" sh -s -
----

#### Side notes for potentially integrating Keepalived with procedures to minimize split-brain

* Assumes a two node, HA K3s cluster
** Each node has Keepalived and MariaDB running under Podman

* Instances of MariaDB use replicatation to sync entire database when one instance joins
* Both instances of MariaDB start and run as read-only

* track_script is used to create additional probes for software on the host that, if return a non-zero exit status, will cause the node to go into a fault state
* *notify_master* script is run after a node successfully becomes the master for an IP instance or IP group
** Can be used to run additional validation elements to prevent split-brain (see below) and potentially amnesia
** There are also *notify_backup* and notify_fault scripts that run when those states are established
** There is also a *notify* script that is run upon any state change
* Doesn't seem to be a way to use keepalived to provoke repeated backups or replicate to an external instance when only one node is in the cluster
* Another way to prevent split-brain could be to update a DNS CNAME for the VIP that shows the master node when only one node is active and the cluster name when two are active.
** *notify_master* could be used to test-set-test the CNAME to establish which node is master
*** The node that fails test-set-test shuts down keepalived

As part of the notify_master script ( triggered when the node takes the VIP), the following command is run to verify that this node can ping the gateway. If so, then mark the local MariaDB instance as r/w:

----
#!/bin/bash

MDB_VIP=$1
NETWORK_GATEWAY=$2
MDB_LOCAL_IP=$3
MDB_ROOT_PASSWORD=$4
K3S_DATABASE=$5

if ping -S ${MDB_VIP} -c 10 ${NETWORK_GATEWAY}; then
	mariadb -h ${MDB_LOCAL_IP} -u root -p${MDB_ROOT_PASSWORD} ${K3S_DATABASE} -e "SET GLOBAL read_only = 0; UNLOCK TABLES;"  #### Needs to provoke a full DB sync to the new backup instance
else
	mariadb -h ${MDB_LOCAL_IP} -u root -p${MDB_ROOT_PASSWORD} ${K3S_DATABASE} -e "FLUSH TABLES WITH READ LOCK; SET GLOBAL read_only = 1;"
fi
----

* From the keepalived.conf file (rough example):

----
  notify_master "/etc/keepalived/keepalived_master.sh 1.2.3.100 1.2.3.1 1.2.3.10 p@ssw0rD k3s
----

* A notify_backup script sets its own instance as RO


## k3sup

* Historically, had tons of trouble creating a cluster manually. K3sup worked well

* Download and install with:
----
curl -sLS https://get.k3sup.dev | sh
sudo install k3sup /usr/local/bin/
----

.Create the custer with the first server(master) node:

----
k3sup install --ip=10.110.2.0 --tls-san=10.110.2.10 --sudo --user sles --cluster --k3s-channel=stable --merge --local-path=$HOME/.kube/config --context=ha-k3ai
----
* Equals signs are optional
* --ip is the server to be installed
* --tls-san ensures the VIP (currently presented by keepalived) is included in the TLS cert
* --sudo means to use sudo for the installation since the SSH user won't be root
* --user is the SSH user account to use on the target system
* --cluster tells the first server node set etcd up in cluster mode
* --merge merges the new cluster context (ha-k3ai in this case) with the existing one in ~/.kube/config

** Note that the KUBECONFIG file won't materialize if there is no file at that location or it can't be parsed correctly

.Adding each additional server(master) node:

----
k3sup join --ip 10.110.2.1 --server --server-ip 10.110.2.10 --k3s-channel stable
----

* --ip is the server to be installed
* --server-ip is the VIP for the K3s API server

.Adding each agent(worker) node:

----
k3sup join --ip 10.110.3.0 --server-ip 10.110.2.10  --sudo --user sles --k3s-channel stable
----

## K3AI

* Command line tool that takes in a specification and deploys a K8s cluster
* Further invocations can add AI frameworks and tools to the deployed cluster
** Runs on Windows, Mac, Linux and limited ARM support
** Can deploy AI to existing cluster
** Not sure if you can deploy the cluster and specific AI tools at the same time
** As of 01/2021 can deploy:
*** Rancher K3s to pre-provisioned nodes
*** Rancher K3d to a docker enabled host
*** Mirantis K0s (have no idea how it works)
*** KinD to a docker enabled host
*** A "remote" cluster


.Outstanding issues: 
