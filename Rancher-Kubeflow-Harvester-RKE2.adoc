### Current state of this doc (04/21/23): Constant flux


IMPORTANT: This guide assumes Rancher Management Server is installed and configured with Harvester under Rancher management.


# Overall steps to complete this task

. Deploy an RKE2 cluster on Harvester
. Deploy the NFS server
. Create the NFS client Container Storage Interface (CSI) on the RKE2 cluster
. Deploy the MetalLB load balancer
. Verify MetalLB and the Longhorn CSI are working correctly
. Deploy Istio
. Prepare the Kubeflow deployment files
. Deploy Kubeflow
. Connect to the Kubeflow UI

# Steps that are currently in moderate flux

NOTE: Many of the following steps will be performed from an AMD/Intel Linux workstation. 

.Prepare the RKE2 Kubernetes cluster
* For this effort, we used Harvester, managed through Rancher, to create a five node RKE2 cluster
* A project in Harvester named kubeflow-on-harvester contains a namespace named kubeflow-cluster

.Create a cluster through the Rancher UI

NOTE: The resource allocations used here were for basic testing purposes. It is likely the more CPU and RAM would be required for the workload-plane VMs to support a useful Kubeflow workload.

* The cluster name set to: 
```sh
kubeflow-on-harvester
```
* Three instances set with the roles of control-plane and etcd pool
** Configure resources of 4 vCPU, 8GB RAM, 40GB boot drive
** Two VMs make up the workload-plane pool:
*** 8 vCPU, 16GB RAM, 40GB boot drive
* All nodes should be in the same Harvester namespace
* The operating systems for all nodes is the SUSE Linux Enterprise 15 SP4 minimal QCOW2 image with cloud-init enabled (previously known as the OpenStack image)
* All nodes are connected to a single VLAN with DHCP, DNS, and routing to the Internet
* The SSH user for the O/S image is 
```sh
sles
```

////
namespace/auth created
namespace/cert-manager created
namespace/istio-system created
namespace/knative-eventing created
namespace/knative-serving created
namespace/kubeflow created
////

* The following User Data cloud-config (under `Show Advanced`) was applied to all nodes during RKE2 cluster creation:
```sh
### cloud-init
#cloud-config
chpasswd:
  list: |
    root:SUSE
    sles:SUSE
  expire: false
ssh_authorized_keys:
  - >-
    <REPLACE WITH SSH PUBLIC KEY OF THE WORKSTATION>
runcmd:
  - SUSEConnect --url <REPLACE WITH RMT SERVER ADDRESS>
  - zypper -n in -t pattern apparmor
  - zypper -n up
  - zypper in --force-resolution --no-confirm --force kernel-default
  - zypper rm kernel-default-base
```
IMPORTANT: These instructions are currently only applicable for Kubernetes versions earlier than 1.25

.The Kubernetes `Cluster Configuration` is as follows:
* On the `Basic` tab:
** Kubernetes version v1.24.9+rke2r2 (currently depricated, but needed for Harvester Cloud Provider support)
** Enable the Harvester Cloud Provider CSI driver
** `Container Network` Interface is Calico
** Set the `Default Security Pod Policy` to `unrestricted`
** Disable the `Nginx Ingress` controller under `System Services`
* On the `Labels and Annotations` tab:
** Apply a cluster label where they key is `app` and the value is `kubeflow` 
* Click `Create`

.Verify and reboot the RKE2 nodes
* After the cluster has been created, SSH to each node as the user `sles`
** Verify that the `kernel-default` kernel has been installed and `kernel-default-base` kernel has been removed: 
```sh
sudo zypper se kernel-default
```

** If needed, remove the `kernel-default-base` kernel with: 
```sh
sudo zypper rm kernel-default-base
```

* Verify that all operating system software has been patched to the latest update: 
```sh
sudo zypper up
```

* Reboot each node, in turn to enable the kernel-default kernel

////
.Prepare NFS server (used master node VM in these tests)
* Basic NFS deployment
* Can use `sudo showmount -e` on the server to verify its serving and `sudo showmount -e <NFS server IP>` from the worker nodes to verify they have everything needed to mount
* Using Kubeflow version 1.6
////


## After the RKE2 cluster has been created, gather the KUBECONFIG data from the Rancher Management server and provide it to a workstation with kubectl and helm installed

## Deploy MetalLB

NOTE: The instructions described below include a section for `Testing MetalLB` after deployment. This can be omitted as both MetalLB and the Harvester CSI will be tested in a later step.

* Use these instructions to deploy MetalLB on the RKE2 cluster: https://gist.github.com/alexarnoldy/24dd06d8c4291d04c5d7065b520bcb15

## Test MetalLB and the Longhorn CSI

* Set this variable with the target namespace: 
```sh
NAMESPACE="kubeflow"
```

* Create the namespace: 
```sh
kubectl create namespace ${NAMESPACE}
```

* Create the manifest for an nginx pod, PVC, and load balancer service:

```sh
cat <<EOF> nginx-metallb-test.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx
  namespace: ${NAMESPACE}
spec:
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1
        ports:
        - name: http
          containerPort: 80
        volumeMounts:
        - mountPath: /mnt/test-vol
          name: test-vol
      volumes:
      - name: test-vol
        persistentVolumeClaim:
          claimName: nginx-pvc


---
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: nginx-pvc
  namespace: ${NAMESPACE}
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi


---
apiVersion: v1
kind: Service
metadata:
  name: nginx
  namespace: ${NAMESPACE}
spec:
  ports:
  - name: http
    port: 8080
    protocol: TCP
    targetPort: 80
  selector:
    app: nginx
  type: LoadBalancer
EOF
```

* Create the pod, service, and the PVC: 
```sh
kubectl apply -f nginx-metallb-test.yaml
```

* Verify the pod is "Running", the `harvester` StorageClass is the `(default)`, the persistentvolumeclaim is "Bound", and the service has an "EXTERNAL-IP": 
```sh
kubectl get pod,sc,pvc,svc -n ${NAMESPACE}
```
* Verify that the service is reachable through the load balancer IP address from outside the cluster:

```sh
IPAddr=$(kubectl get svc -n ${NAMESPACE} | grep -w nginx | awk '{print$4":"$5}' | awk -F: '{print$1":"$2}')
curl http://${IPAddr} | grep "Thank you"
```

** An HTML encoded output should display the phrase "Thank you for using nginx."

* Verify that the volume is mounted in the test pod: 

```sh
TEST_POD=$(kubectl get pods -n ${NAMESPACE} | awk '/nginx/ {print$1}')
kubectl exec -it ${TEST_POD} -n ${NAMESPACE} -- mount | grep test-vol
```
** The output should show that the volume is mounted at the location `/mnt/test-vol` 

* When finished with testing, delete the pod and service: 
```sh
kubectl delete -f nginx-metallb-test.yaml
```


////

*****Omitted in favor of Longhorn*****
.Deploy nfs-client-provisioner from Helm catalog
* From top menu bar, point to "Global" or the cluster name, then point to the cluster name just below it, then select "Default" project
* From top menu bar, select "Apps", then select "Launch"
* Search for "nfs-client-provisioner", then select it
** Under "Answers", paste the following into the first "Variable" answer box:
----
nfs.server=IPAddress
nfs.path=FullyQualifiedPath
storageClass.name=nfs
storageClass.defaultClass=true
----
*** Replace "IPAddress" with the hostname or IP address of the NFS server (RKE master node in these tests)
*** Replace "FullyQualifiedPath" with the fully qualified path of the NFS share
* Select "Launch" at the bottom of the page

////



////
===== Enable Istio

NOTE: This guide assumes Istio was not installed when the RKE2 cluster was instantiated. 

NOTE: Installing Istio through the Rancher App Catalog requires that Rancher Monitoring be installed first.

.Install Rancher Montoring via the Rancher Manager UI
* From the "Cluster Managment" GLOBAL APP, select "Explore" on the target RKE2 cluster
* Select "Cluster Tools", then click on "Install" for `Monitoring`
* Select the appropriate version of the Rancher chart (the latest was used for this guide)
* Install into the "System" project
* Click `Next`
* Accept the default settings on the next page
* Click `Install`
* The installation will open a terminal window in the bottom section of the Rancher Manager UI
* Keep that terminal window open until it completes with an output that includes: `SUCCESS: helm upgrade --install=true ...`

.Install Istio via the Rancher Manager UI
* From the "Cluster Managment" GLOBAL APP, select "Explore" on the target RKE2 cluster
* Select "Cluster Tools", then click on "Install" for `Istio`
* Select the appropriate version of the Rancher chart (the latest was used for this guide)
* Install into the "System" project
* Select `Customize Helm options before install`
* Click `Next`
* Accept the default Components on the next page
* Click `Edit YAML`
** In the YAML file, change ingressGateways.type to `LoadBalancer`
* Click `Install`
* The installation will open a terminal window in the bottom section of the Rancher Manager UI
* Keep that terminal window open until it completes with an output that includes: `SUCCESS: helm install --namespace=istio-system ...`
////

////
** (Optionally) Provide an IP address that is assigned to MetalLB but not in use
////

////
******Don't think this is a very good test at this point******

* Use kubectl on the workstation CLI to Validate the istio-ingressgateway has received an IP address: `kubectl get svc -A | egrep --color 'EXTERNAL-IP|LoadBalancer'`
** (Optionally) Validate an external connection to an internal Istio service: 
*** Use the curl command to connect to a few of the *PORT(S)* listed for the istio-ingressgateway, i.e. `

----
kubectl get svc -n istio-system istio-ingressgateway | awk '{print$5}'
curl http://{$IPADDR}:15020
----

*** At least one of the ports should return "404 page not found"
////

## Install Kubeflow

NOTE: The instructions for installing Kubeflow can be found at: `https://github.com/kubeflow/manifests#installation`

.Install kustomize on the Linux workstation:

IMPORTANT: At the time of writing, Kubeflow requires kustomize version 5.0.0 or higher

* Find the lastest release of kustomize at https://github.com/kubernetes-sigs/kustomize/releases/
* Adjust this variable for the appropriate release: `VERSION="v5.0.0"`
*** Use the following commands to download and install kustomize for a Linux AMD/Intel workstation:

```sh
wget https://github.com/kubernetes-sigs/kustomize/releases/download/kustomize%2F${VERSION}/kustomize_${VERSION}_linux_amd64.tar.gz
tar xvfz kustomize_${VERSION}_linux_amd64.tar.gz
sudo mv kustomize /usr/bin
```
* Follow the instructions at `https://github.com/kubeflow/manifests#installation` to either install all of the Kubeflow components with a single command, or install individual components

NOTE: MetalLB is configured to provide a Virtual IP address to `istio-ingressgateway`. However, as described at this page (https://github.com/kubeflow/manifests#nodeport--loadbalancer--ingress), TLS must be configured first. After TLS has been configured, the `istio-ingressgateway` service in the `istio-system` namespace can be edited to change the service type from `ClusterIP` to `LoadBalancer`.


