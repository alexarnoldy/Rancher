### Current state of this doc (05/17/23): Constant flux


IMPORTANT: This guide assumes Rancher Management Server is installed and configured with Harvester under Rancher management.



# Overall steps to complete this task

## UPDATE START

. Deploy an RKE2 cluster on Harvester
. Deploy the MetalLB load balancer
. Verify MetalLB and the Longhorn CSI are working correctly
. Deploy Kubeflow
. Update Kubeflow to use the MetalLB load balancer
. Connect to the Kubeflow UI via HTTP
. Configure cert-manager to obtain a TLS certificate from Let's Encrypt
. Configure Istio to use the TLS certificate
. Connect to the Kubeflow UI via HTTPS

NOTE: Many of the following steps will be performed from an AMD/Intel Linux workstation with access to Rancher, Harvester, and the Internet.


## UPDATE END

.Prepare the RKE2 Kubernetes cluster
* For this effort, we used Harvester, managed through Rancher, to create a five node RKE2 cluster
* A project in Harvester named kubeflow-on-harvester contains a namespace named kubeflow-cluster

.Create a cluster through the Rancher UI

NOTE: The resource allocations used here were for basic testing purposes. It is likely the more CPU and RAM would be required for the workload-plane VMs to support a useful Kubeflow workload.

* The cluster name set to: 
```sh
kubeflow-on-harvester
```
* Three instances set with the roles of control-plane and etcd pool
** Configure resources of 4 vCPU, 8GB RAM, 40GB boot drive
** Two VMs make up the workload-plane pool:
*** 8 vCPU, 16GB RAM, 40GB boot drive

## UPDATE START

* Any Harvester namespace
* The `Image Volume` operating systems for all nodes is the SUSE Linux Enterprise 15 SP4 minimal QCOW2 image with cloud-init enabled (previously known as the OpenStack image)
* All nodes are connected to a Harvester network is connect to a VLAN with DHCP, DNS, and routing to the Internet

## UPDATE END

* The SSH user for the O/S image is 
```sh
sles
```


## UPDATE START


* The following `User Data` cloud-config (under `Show Advanced`) was applied to all nodes during RKE2 cluster creation:

## UPDATE END

```sh
### cloud-init
#cloud-config
chpasswd:
  list: |
    root:SUSE
    sles:SUSE
  expire: false
ssh_authorized_keys:
  - >-
    <REPLACE WITH SSH PUBLIC KEY OF THE WORKSTATION>
runcmd:
  - SUSEConnect --url <REPLACE WITH RMT SERVER ADDRESS>
  - zypper -n in -t pattern apparmor
  - zypper -n up
  - zypper in --force-resolution --no-confirm --force kernel-default

## UPDATE START

  - zypper rm --no-confirm kernel-default-base

## UPDATE END

```

## UPDATE START

* Select the tick-box to `Install guest agent`

## UPDATE END


IMPORTANT: These instructions are currently only applicable for Kubernetes versions earlier than 1.25

.The Kubernetes `Cluster Configuration` is as follows:
* On the `Basic` tab:
** Kubernetes version v1.24.9+rke2r2 (currently depricated, but needed for Harvester Cloud Provider support)
** Enable the Harvester Cloud Provider CSI driver
** `Container Network` Interface is Calico

## UPDATE START

** Ensure the `Default Security Pod Policy` is set to `Default - RKE2 Embedded`

## UPDATE END

** Disable the `Nginx Ingress` controller under `System Services`

* On the `Labels and Annotations` tab:

** Apply a cluster label where they key is `app` and the value is `kubeflow` 
* Click `Create`

.Verify and reboot the RKE2 nodes
* After the cluster has been created, SSH to each node as the user `sles`
** Verify that the `kernel-default` kernel has been installed and `kernel-default-base` kernel has been removed: 
```sh
sudo zypper se kernel-default
```

** If needed, remove the `kernel-default-base` kernel with: 
```sh

## UPDATE START

sudo zypper rm --no-confirm kernel-default-base

## UPDATE END

```

* Verify that all operating system software has been patched to the latest update: 
```sh
sudo zypper up
```

* Reboot each node, in turn to enable the kernel-default kernel

## UPDATE START

```sh
sudo reboot
```

## UPDATE END



## After the RKE2 cluster has been created, gather the KUBECONFIG data from the Rancher Management server and provide it to a workstation with kubectl and helm installed

## Deploy MetalLB

NOTE: The instructions described below include a section for `Testing MetalLB` after deployment. This can be omitted as both MetalLB and the Harvester CSI will be tested in a later step.

* Use these instructions to deploy MetalLB on the RKE2 cluster: https://gist.github.com/alexarnoldy/24dd06d8c4291d04c5d7065b520bcb15

## Test MetalLB and the Havester (Longhorn) CSI

* Set this variable with the target namespace: 
```sh
NAMESPACE="kubeflow"
```

* Create the namespace: 
```sh
kubectl create namespace ${NAMESPACE}
```

* Create the manifest for an nginx pod, PVC, and load balancer service:

```sh
cat <<EOF> nginx-metallb-test.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx
  namespace: ${NAMESPACE}
spec:
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1
        ports:
        - name: http
          containerPort: 80
        volumeMounts:
        - mountPath: /mnt/test-vol
          name: test-vol
      volumes:
      - name: test-vol
        persistentVolumeClaim:
          claimName: nginx-pvc


---
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: nginx-pvc
  namespace: ${NAMESPACE}
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi


---
apiVersion: v1
kind: Service
metadata:
  name: nginx
  namespace: ${NAMESPACE}
spec:
  ports:
  - name: http
    port: 8080
    protocol: TCP
    targetPort: 80
  selector:
    app: nginx
  type: LoadBalancer
EOF
```

* Create the pod, service, and the PVC: 
```sh
kubectl apply -f nginx-metallb-test.yaml
```

* Verify the pod is "Running", the `harvester` StorageClass is the `(default)`, the persistentvolumeclaim is "Bound", and the service has an "EXTERNAL-IP": 
```sh
kubectl get pod,sc,pvc,svc -n ${NAMESPACE}
```
* Verify that the service is reachable through the load balancer IP address from outside the cluster:

```sh
IPAddr=$(kubectl get svc -n ${NAMESPACE} | grep -w nginx | awk '{print$4":"$5}' | awk -F: '{print$1":"$2}')
curl http://${IPAddr} | grep "Thank you"
```

** An HTML encoded output should display the phrase "Thank you for using nginx."

* Verify that the volume is mounted in the test pod: 

```sh
TEST_POD=$(kubectl get pods -n ${NAMESPACE} | awk '/nginx/ {print$1}')
kubectl exec -it ${TEST_POD} -n ${NAMESPACE} -- mount | grep test-vol
```
** The output should show that the volume is mounted at the location `/mnt/test-vol` 

* When finished with testing, delete the pod and service: 
```sh
kubectl delete -f nginx-metallb-test.yaml

## UPDATE START

sleep 5
kubectl delete ns ${NAMESPACE}

## UPDATE END

```


////

*****Omitted in favor of Longhorn*****
.Deploy nfs-client-provisioner from Helm catalog
* From top menu bar, point to "Global" or the cluster name, then point to the cluster name just below it, then select "Default" project
* From top menu bar, select "Apps", then select "Launch"
* Search for "nfs-client-provisioner", then select it
** Under "Answers", paste the following into the first "Variable" answer box:
----
nfs.server=IPAddress
nfs.path=FullyQualifiedPath
storageClass.name=nfs
storageClass.defaultClass=true
----
*** Replace "IPAddress" with the hostname or IP address of the NFS server (RKE master node in these tests)
*** Replace "FullyQualifiedPath" with the fully qualified path of the NFS share
* Select "Launch" at the bottom of the page

////



////
===== Enable Istio

NOTE: This guide assumes Istio was not installed when the RKE2 cluster was instantiated. 

NOTE: Installing Istio through the Rancher App Catalog requires that Rancher Monitoring be installed first.

.Install Rancher Montoring via the Rancher Manager UI
* From the "Cluster Managment" GLOBAL APP, select "Explore" on the target RKE2 cluster
* Select "Cluster Tools", then click on "Install" for `Monitoring`
* Select the appropriate version of the Rancher chart (the latest was used for this guide)
* Install into the "System" project
* Click `Next`
* Accept the default settings on the next page
* Click `Install`
* The installation will open a terminal window in the bottom section of the Rancher Manager UI
* Keep that terminal window open until it completes with an output that includes: `SUCCESS: helm upgrade --install=true ...`

.Install Istio via the Rancher Manager UI
* From the "Cluster Managment" GLOBAL APP, select "Explore" on the target RKE2 cluster
* Select "Cluster Tools", then click on "Install" for `Istio`
* Select the appropriate version of the Rancher chart (the latest was used for this guide)
* Install into the "System" project
* Select `Customize Helm options before install`
* Click `Next`
* Accept the default Components on the next page
* Click `Edit YAML`
** In the YAML file, change ingressGateways.type to `LoadBalancer`
* Click `Install`
* The installation will open a terminal window in the bottom section of the Rancher Manager UI
* Keep that terminal window open until it completes with an output that includes: `SUCCESS: helm install --namespace=istio-system ...`
////

////
** (Optionally) Provide an IP address that is assigned to MetalLB but not in use
////

////
******Don't think this is a very good test at this point******

* Use kubectl on the workstation CLI to Validate the istio-ingressgateway has received an IP address: `kubectl get svc -A | egrep --color 'EXTERNAL-IP|LoadBalancer'`
** (Optionally) Validate an external connection to an internal Istio service: 
*** Use the curl command to connect to a few of the *PORT(S)* listed for the istio-ingressgateway, i.e. `

----
kubectl get svc -n istio-system istio-ingressgateway | awk '{print$5}'
curl http://{$IPADDR}:15020
----

*** At least one of the ports should return "404 page not found"
////

## Install Kubeflow

NOTE: The instructions for installing Kubeflow can be found at: `https://github.com/kubeflow/manifests#installation`

.Install kustomize on the Linux workstation:

IMPORTANT: At the time of writing, Kubeflow requires kustomize version 5.0.0 or higher

* Find the lastest release of kustomize at https://github.com/kubernetes-sigs/kustomize/releases/
* Adjust this variable for the appropriate release: `VERSION="v5.0.0"`
*** Use the following commands to download and install kustomize for a Linux AMD/Intel workstation:

```sh
wget https://github.com/kubernetes-sigs/kustomize/releases/download/kustomize%2F${VERSION}/kustomize_${VERSION}_linux_amd64.tar.gz
tar xvfz kustomize_${VERSION}_linux_amd64.tar.gz
sudo mv kustomize /usr/bin
```

## Install Kubeflow
NOTE: The remainder of the procedure will require installing Kubeflow according to the instructions on the Kubeflow GitHub site, then returning to this document to enable TLS for HTTPS connections to the Kubeflow Dashboard.

* Follow the instructions at `https://github.com/kubeflow/manifests#installation` to either install all of the Kubeflow components with a single command, or install individual components

## Enable TLS (HTTPS) on Kubeflow

## UPDATE START - Removed creating the TLS certificate and adding to the Istio gateway

////
* Follow the instructions at `https://gist.github.com/alexarnoldy/e84216fa969f849e79ffbcf766e92ffc` to create an applicatoin TLS certificate and private key
** Be sure to use these variables, while following this procudure:

```sh
export APP="kubeflow"
export APP_K8S_NAMESPACE="istio-system"
export APP_SERVICE_NAME="istio-ingressgateway"
export SECRET_NAME="kubeflow-tls"
```
.After the TLS secret has been created, add the required TLS information to Istio:

* Edit the `istio-ingressgateway` gateway: 

```sh
kubectl edit gateway -n istio-system istio-ingressgateway 
```

* Add this information at the bottom of the manifest, after the "host" HTTP information:

```sh
    tls:
      httpsRedirect: true
  - hosts:
    - '*'
    port:
      name: https
      number: 443
      protocol: HTTPS
    tls:
      credentialName: kubeflow-lb-tls
      mode: SIMPLE
```
* The `spec` section of the manifest should resemble:

----
spec:
  selector:
    app: istio-ingressgateway
    istio: ingressgateway
  servers:
  - hosts:
    - '*'
    port:
      name: http
      number: 80
      protocol: HTTP
  - hosts:
    - '*'
    port:
      name: https
      number: 443
      protocol: HTTPS
    tls:
      credentialName: kubeflow-lb-tls
      mode: SIMPLE
----

NOTE: The `credentialName` must match the TLS secret created earlier.
////

## UPDATE END

## UPDATE START

## Patch the `istio-ingressgateway` service to change the service type to LoadBalancer:

## UPDATE END

* Verify the current service type:

```sh
kubectl -n istio-system get svc istio-ingressgateway -o jsonpath='{.spec.type}' ; echo ""
```

* Patch the service to change the type to LoadBalancer:

```sh
kubectl -n istio-system patch svc istio-ingressgateway -p '{"spec": {"type": "LoadBalancer"}}'
```

* Verify the service is a type of `LoadBalancer`and gather the IP address:
```sh
kubectl -n istio-system get svc istio-ingressgateway
```

## UPDATE START


## Use a browser to connect with HTTP (not HTTPS) to Kubeflow UI 

* The screen should redirect to dex and offer a login prompt
* Login with the credentials: 
`Email address`

```sh
user@example.com
```

`Password`
```sh
12341234
```

IMPORTANT: Proceed to the next section only after being able to log into the Kubeflow UI

## Configure cert-manager to manage certificates from Let's Encrypt, using Route 53 DNS records

NOTE: cert-manager can manage certificates from any public DNS provider. See the cert-manager documentation at https://cert-manager.io/docs/configuration/acme/ for more information.

NOTE: An AWS user with appropriate IAM policies and API access keys is needed for cert-manager to access the Route53 DNS records. See the cert-manager documentation at https://cert-manager.io/docs/configuration/acme/dns01/route53/ for more information.

.Create a cert-manager Issuer for Let's Encrypt:
* Set these variables:
```sh
# aws_access_key_id and aws_secret_access_key for the configured AWS user:
export AWS_ACCESS_KEY_ID=""
export AWS_SECRET_ACCESS_KEY=""
export AWS_REGION="" # E.g. "us-west-2"
export DNSZONE="" # E.g. "suse.com"
export FQDN="" # E.g. "kubeflow.suse.com"
export EMAIL_ADDR="" # valid email address for the Let's Encrypt certificate

```

NOTE: When initially creating the cert-manager Issuer, ensure the `server: https://acme-staging-v02...` line is uncommented and the `server: https://acme-v02...` line is commented out. After verifying that the certicate can be issued correctly, we will reverse this to obtain the valid, production certificate.

* Create the cert-manager Issuer file:
```sh
cat <<EOF> letsencrypt-issuer.yaml
apiVersion: cert-manager.io/v1
kind: Issuer
metadata:
  name: letsencrypt-issuer
  namespace: istio-system
spec:
  acme:
    email: ${EMAIL_ADDR}
    server: https://acme-staging-v02.api.letsencrypt.org/directory # Use this line to test the process of issuing a certificate to avoid the Let's Encrypt production rate limits
#    server: https://acme-v02.api.letsencrypt.org/directory # Use this line after the certificate issues correctly
    privateKeySecretRef:
      name: letsencrypt-issuer-priv-key # K8s secret that will contain the private key for this, specific issuer
    solvers:
    - selector:
        dnsZones: 
          - "${DNSZONE}"
      dns01:
        route53:
          region: ${AWS_REGION}
          accessKeyID: ${AWS_ACCESS_KEY_ID}
          secretAccessKeySecretRef:
            name: route53-credentials-secret
            key: secret-access-key
EOF
```

* Create the Kubernetes secret containing the aws_secret_access_key for the AWS user:
```sh
kubectl create -n istio-system secret generic route53-credentials-secret --from-literal=secret-access-key=${AWS_SECRET_ACCESS_KEY}```

* Verify the contents of the secret:
```sh
kubectl get -n istio-system secret route53-credentials-secret -o jsonpath={.data.secret-access-key} | base64 -d; echo ""
```

## Update OIDC to allow the Let's Encrypt DNS01 challenge:

* From inside the Kubeflow `manifests` directory (i.e. the base directory from the cloned https://github.com/kubeflow/manifests repository), update the oidc-authservice params.env file:

```sh
sed -i 's/SKIP_AUTH_URI=\/dex/SKIP_AUTH_URI=\/dex \/.well-known/' common/oidc-authservice/base/params.env
```

* Verify the update to the file:
```sh
cat common/oidc-authservice/base/params.env
```

* Update the running oidc-authservice instance:
```sh
kustomize build common/oidc-authservice/base | kubectl apply -f -
```

* Verify the hostname for the certificate resolves correctly:
```sh
getent hosts ${FQDN}
```

* Create the cert-manager Certificate resource file:
```sh
cat <<EOF> kubeflow-certificate.yaml 
apiVersion: cert-manager.io/v1
kind: Certificate
metadata:
  name: kubeflow-certificate
  namespace: istio-system
spec: 
  secretName: kubeflow-certificate-secret # Kubernetes secret that will contain the tls.key and tls.crt of the new cert
  commonName: ${FQDN}
  dnsNames:
    - ${FQDN}
  issuerRef:
    name: letsencrypt-issuer
    kind: Issuer
EOF
```

* Verify the Certificate resource file:
```sh
cat kubeflow-certificate.yaml
```

* Create the Certificate resource:
```sh
kubectl apply -f kubeflow-certificate.yaml
```

* Check the status of the certificate:
```sh
kubectl get -n istio-system certificate
```

NOTE: The certificate can take up to three minutes to be issued, as indicated by the `READY` status becoming `True`


* If needed, check the progress of the certificate:
```sh
kubectl describe -n istio-system certificate kubeflow-certificate
```

IMPORTANT: If the certificate seems to be taking a long time to be issued, review the cert-manager logs for clues. Common errors are related to DNS resolution, credentials, and IAM policies. Keep checking back for the status of the certificate since it will likely keep working in the background. 

* If needed, review the cert-manager logs:
```sh
kubectl logs -n cert-manager -l app=cert-manager
```

IMPORTANT: Proceed to the next section only after the certificate shows a `READY` status of `True` 

## Update the gateway to use TLS with the cert

## Use a browser to connect with HTTPS to Kubeflow UI 

NOTE: Since the certificate was issued by the Let's Encrypt Staging servers, it will cause an error in the browser that it is untrusted. 

* Click the lock icon in the browser's URL pane, then continue selecting appropriate options until you are able to review the connection certificate. It should say that the certificate was issued by Let's Encrypt (Staging)

* The screen should redirect to dex and offer a login prompt

* After ready becomes true (about 3 minutes):  
** change issuer from staging to prod 
** delete the secret containing the cert and key
** recreate the cert


## UPDATE END

////
NOTE: MetalLB is configured to provide a Virtual IP address to `istio-ingressgateway`. However, as described at this page (https://github.com/kubeflow/manifests#nodeport--loadbalancer--ingress), TLS must be configured first. After TLS has been configured, the `istio-ingressgateway` service in the `istio-system` namespace can be edited to change the service type from `ClusterIP` to `LoadBalancer`.
////

