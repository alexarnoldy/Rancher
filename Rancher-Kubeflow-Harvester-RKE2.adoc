### Current state of this doc (05/17/23): Constant flux


IMPORTANT: This guide assumes Rancher Management Server is installed and configured with Harvester under Rancher management.



# Overall steps to complete this task

## UPDATE START

. Deploy an RKE2 cluster on Harvester
. Deploy the MetalLB load balancer
. Verify MetalLB and the Longhorn CSI are working correctly
. Deploy Kubeflow
. Update Kubeflow to use the MetalLB load balancer
. Connect to the Kubeflow UI via HTTP
. Configure cert-manager to obtain a TLS certificate from Let's Encrypt
. Configure Istio to use the TLS certificate
. Connect to the Kubeflow UI via HTTPS

NOTE: Many of the following steps will be performed from an AMD/Intel Linux workstation with access to Rancher, Harvester, and the Internet.


## UPDATE END

.Prepare the RKE2 Kubernetes cluster
* For this effort, we used Harvester, managed through Rancher, to create a five node RKE2 cluster
* A project in Harvester named kubeflow-on-harvester contains a namespace named kubeflow-cluster

.Create a cluster through the Rancher UI

NOTE: The resource allocations used here were for basic testing purposes. It is likely the more CPU and RAM would be required for the workload-plane VMs to support a useful Kubeflow workload.

* The cluster name set to: 
```sh
kubeflow-on-harvester
```
* Three instances set with the roles of control-plane and etcd pool
** Configure resources of 4 vCPU, 8GB RAM, 40GB boot drive
** Two VMs make up the workload-plane pool:
*** 8 vCPU, 16GB RAM, 40GB boot drive

## UPDATE START

* Any Harvester namespace
* The `Image Volume` operating systems for all nodes is the SUSE Linux Enterprise 15 SP4 minimal QCOW2 image with cloud-init enabled (previously known as the OpenStack image)
* All nodes are connected to a Harvester network is connect to a VLAN with DHCP, DNS, and routing to the Internet

## UPDATE END

* The SSH user for the O/S image is 
```sh
sles
```


## UPDATE START


* The following `User Data` cloud-config (under `Show Advanced`) was applied to all nodes during RKE2 cluster creation:

## UPDATE END

```sh
### cloud-init
#cloud-config
chpasswd:
  list: |
    root:SUSE
    sles:SUSE
  expire: false
ssh_authorized_keys:
  - >-
    <REPLACE WITH SSH PUBLIC KEY OF THE WORKSTATION>
runcmd:

## UPDATE START

#  - SUSEConnect --url <REPLACE WITH RMT SERVER ADDRESS>                                               # Uncomment if an RMT server is available
#  - SUSEConnect -e <REPLACE WITH REGISTERED EMAILL ADDRESS> -r <REPLACE WITH SCC SUBSCRIPTION KEY>    # Uncomment if using an SCC subscription key

## UPDATE END

  - zypper -n in -t pattern apparmor
  - zypper -n up
  - zypper in --force-resolution --no-confirm --force kernel-default

## UPDATE START

  - zypper rm --no-confirm kernel-default-base

## UPDATE END

```

## UPDATE START

* Select the tick-box to `Install guest agent`

## UPDATE END


IMPORTANT: These instructions are currently only applicable for Kubernetes versions earlier than 1.25

.The Kubernetes `Cluster Configuration` is as follows:
* On the `Basic` tab:
** Kubernetes version v1.24.9+rke2r2 (currently depricated, but needed for Harvester Cloud Provider support)
** Enable the Harvester Cloud Provider CSI driver
** `Container Network` Interface is Calico

## UPDATE START

** Ensure the `Default Security Pod Policy` is set to `Default - RKE2 Embedded`
** Leave `Pod Security Admission Configuration Template` set to `(None)`

## UPDATE END

* (Ignore this line, it is needed to fix bullet points below)
** Disable the `Nginx Ingress` controller under `System Services`

* On the `Labels and Annotations` tab:

** Apply a cluster label where they key is `platform` and the value is `kubeflow` 
* Click `Create`

.Verify and reboot the RKE2 nodes
* After the cluster has been created, SSH to each node as the user `sles`
** Verify that the `kernel-default` kernel has been installed and `kernel-default-base` kernel has been removed: 
```sh
sudo zypper se kernel-default
```

** If needed, remove the `kernel-default-base` kernel with: 
```sh

## UPDATE START

sudo zypper rm --no-confirm kernel-default-base

## UPDATE END

```

* Verify that all operating system software has been patched to the latest update: 
```sh
sudo zypper up
```

* Reboot each node, in turn to enable the kernel-default kernel

## UPDATE START

```sh
sudo reboot
```

## UPDATE END



## After the RKE2 cluster has been created, gather the KUBECONFIG data from the Rancher Management server and provide it to a workstation with kubectl and helm installed

## Deploy MetalLB

NOTE: The instructions described below include a section for `Testing MetalLB` after deployment. This can be omitted as both MetalLB and the Harvester CSI will be tested in a later step.

* Use these instructions to deploy MetalLB on the RKE2 cluster: https://gist.github.com/alexarnoldy/24dd06d8c4291d04c5d7065b520bcb15

## Test MetalLB and the Havester (Longhorn) CSI

* Set this variable with the target namespace: 
```sh
NAMESPACE="lb-test"
```

* Create the namespace: 
```sh
kubectl create namespace ${NAMESPACE}
```

* Create the manifest for an nginx pod, PVC, and load balancer service:

```sh
cat <<EOF> nginx-metallb-test.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx
  namespace: ${NAMESPACE}
spec:
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1
        ports:
        - name: http
          containerPort: 80
        volumeMounts:
        - mountPath: /mnt/test-vol
          name: test-vol
      volumes:
      - name: test-vol
        persistentVolumeClaim:
          claimName: nginx-pvc


---
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: nginx-pvc
  namespace: ${NAMESPACE}
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi


---
apiVersion: v1
kind: Service
metadata:
  name: nginx
  namespace: ${NAMESPACE}
spec:
  ports:
  - name: http
    port: 8080
    protocol: TCP
    targetPort: 80
  selector:
    app: nginx
  type: LoadBalancer
EOF
```

* Create the pod, service, and the PVC: 
```sh
kubectl apply -f nginx-metallb-test.yaml
```

* Verify the pod is "Running", the `harvester` StorageClass is the `(default)`, the persistentvolumeclaim is "Bound", and the service has an "EXTERNAL-IP": 
```sh
kubectl get pod,sc,pvc,svc -n ${NAMESPACE}
```
* Verify that the service is reachable through the load balancer IP address from outside the cluster:


## UPDATE START

```sh
IPAddr=$(kubectl get svc -n ${NAMESPACE} | grep -w nginx | awk '{print$4":"$5}' | awk -F: '{print$1":"$2}')
curl http://${IPAddr} 2>/dev/null | grep "Thank you for using nginx"
```

## UPDATE END


** An HTML encoded output should display the phrase "Thank you for using nginx."

* Verify that the volume is mounted in the test pod: 

```sh
TEST_POD=$(kubectl get pods -n ${NAMESPACE} | awk '/nginx/ {print$1}')
kubectl exec -it ${TEST_POD} -n ${NAMESPACE} -- mount | grep test-vol
```
** The output should show that the volume is mounted at the location `/mnt/test-vol` 

* When finished with testing, delete the pod and service: 
```sh
kubectl delete -f nginx-metallb-test.yaml

## UPDATE START

sleep 5
kubectl delete namespace ${NAMESPACE}

## UPDATE END

```




////
===== Enable Istio

NOTE: This guide assumes Istio was not installed when the RKE2 cluster was instantiated. 

NOTE: Installing Istio through the Rancher App Catalog requires that Rancher Monitoring be installed first.

.Install Rancher Montoring via the Rancher Manager UI
* From the "Cluster Managment" GLOBAL APP, select "Explore" on the target RKE2 cluster
* Select "Cluster Tools", then click on "Install" for `Monitoring`
* Select the appropriate version of the Rancher chart (the latest was used for this guide)
* Install into the "System" project
* Click `Next`
* Accept the default settings on the next page
* Click `Install`
* The installation will open a terminal window in the bottom section of the Rancher Manager UI
* Keep that terminal window open until it completes with an output that includes: `SUCCESS: helm upgrade --install=true ...`

.Install Istio via the Rancher Manager UI
* From the "Cluster Managment" GLOBAL APP, select "Explore" on the target RKE2 cluster
* Select "Cluster Tools", then click on "Install" for `Istio`
* Select the appropriate version of the Rancher chart (the latest was used for this guide)
* Install into the "System" project
* Select `Customize Helm options before install`
* Click `Next`
* Accept the default Components on the next page
* Click `Edit YAML`
** In the YAML file, change ingressGateways.type to `LoadBalancer`
* Click `Install`
* The installation will open a terminal window in the bottom section of the Rancher Manager UI
* Keep that terminal window open until it completes with an output that includes: `SUCCESS: helm install --namespace=istio-system ...`
////

////
** (Optionally) Provide an IP address that is assigned to MetalLB but not in use
////

////
******Don't think this is a very good test at this point******

* Use kubectl on the workstation CLI to Validate the istio-ingressgateway has received an IP address: `kubectl get svc -A | egrep --color 'EXTERNAL-IP|LoadBalancer'`
** (Optionally) Validate an external connection to an internal Istio service: 
*** Use the curl command to connect to a few of the *PORT(S)* listed for the istio-ingressgateway, i.e. `

----
kubectl get svc -n istio-system istio-ingressgateway | awk '{print$5}'
curl http://{$IPADDR}:15020
----

*** At least one of the ports should return "404 page not found"
////


## UPDATE START

## Prepare to Install Kubeflow

## UPDATE END


NOTE: The instructions for installing Kubeflow can be found at: `https://github.com/kubeflow/manifests#installation`


## UPDATE START

IMPORTANT: At the time of writing, Kubeflow requires kustomize version 5.0.0 or higher


.Install kustomize 5.0.0 or higher on the Linux workstation:

## UPDATE END

* Find the lastest release of kustomize at https://github.com/kubernetes-sigs/kustomize/releases/
* Adjust this variable for the appropriate release: `VERSION="v5.0.0"`
*** Use the following commands to download and install kustomize for a Linux AMD/Intel workstation:

```sh
wget https://github.com/kubernetes-sigs/kustomize/releases/download/kustomize%2F${VERSION}/kustomize_${VERSION}_linux_amd64.tar.gz
tar xvfz kustomize_${VERSION}_linux_amd64.tar.gz
sudo mv kustomize /usr/bin
```

## Install Kubeflow
NOTE: The remainder of the procedure will require installing Kubeflow according to the instructions on the Kubeflow GitHub site, then returning to this document to enable TLS for HTTPS connections to the Kubeflow Dashboard.


## UPDATE START

IMPORTANT: Before running the first installation command, it is recommended to run `git status` in the `manifests` directory to ensure no unexpected changes have been made to this copy of the git repo. Additionally, it is recommeneded to remove the `manifests` directory and re-clone the repo between installation efforts.

* Clone the repository at https://github.com/kubeflow/manifests, change into the manifests directory, then follow the instructions to either install all of the Kubeflow components with a single command, or install individual components

NOTE: The remainder of this procedure has only been tested with an full installation (E.i. https://github.com/kubeflow/manifests#install-with-a-single-command)

## Verify the Kubeflow installation:

* Ensure all pods have a `STATUS` of `Running` and all of the containers in each pod (E.g. 1/1, not 1/2 or 0/1) are running:
```sh
for EACH in auth cert-manager istio-system knative-eventing knative-serving kubeflow kubeflow-user-example-com; do kubectl get pods -n ${EACH}; read -p "<Enter to continue>"; echo ""; done
```
* Enable kubectl port-forwarding and ensure the Kubeflow UI permits login:
```sh
kubectl port-forward svc/istio-ingressgateway -n istio-system 8080:80
```
* In a browser on the Linux workstation, connect to:
```sh
http://127.0.0.1:8080
```

* Login with the credentials: 
`Email address`

```sh
user@example.com
```

`Password`
```sh
12341234
```

* Use `Ctrl+c` to close the kubectl port-forward session


### Troubleshooting Kubeflow installation
* Some things that could prevent connecting or loggging into the Kubeblow dashboard include:
. The local copy of the https://github.com/kubeflow/manifests git repo doesn't match the origin
** While in the `manifests` directory, run `git status` to see if any files are different from the origin repo
** Remove the `manifests` directory and clone the repo again
. Using a web browser that is not running on the Linux desktop
** The kubectl port-forwarding opens a tunnel from the Linux workstation to the Kubeflow gateway service that only a web browser running on the same system can utilize.
. The Kubeflow installation has not completed or failed to complete
** Return to the beginning of this `Verify the Kubeflow installation` section and ensure all containers and pods are running correctly
** A high number of container restarts can indicate other issues preventing the installation from completing sucessfully
. The cluster's resources are saturated
** Use the Linux `top` command on the worker nodes to ensure the system's CPU/memory are not overburdened
** Check the Harvester dashboard to ensure the physical Harvester nodes are not overburdened or experiencing failures



## UPDATE END


## Enable TLS (HTTPS) on Kubeflow

## UPDATE START 
(The procedure to create a TLS certificate and add it to the Istio gateway has been removed) 
## UPDATE END




## UPDATE START

## Patch the `istio-ingressgateway` service to change the service type to LoadBalancer:

## UPDATE END


## UPDATE START

* Verify the current service type (Likely `ClusterIP`):

```sh
kubectl -n istio-system get svc istio-ingressgateway -o jsonpath='{.spec.type}' ; echo ""
```

* Patch the service to change the type to LoadBalancer:

```sh
kubectl -n istio-system patch svc istio-ingressgateway -p '{"spec": {"type": "LoadBalancer"}}'
```

* Verify the service is a type of `LoadBalancer` and take note of the IP address:
```sh
kubectl -n istio-system get svc istio-ingressgateway
```

## Update the Kubeflow Istio Gateway

* Edit the kubeflow-gateway resource to add HTTPS routing:
```sh
kubectl edit -n kubeflow gateways.networking.istio.io kubeflow-gateway
```

* Add this portion to the bottom of the `spec:` section:

```sh
    tls:
      httpsRedirect: false
  - hosts:
    - "*"
    port:
      name: https
      number: 443
      protocol: HTTPS
    tls:
      mode: SIMPLE
      credentialName: kubeflow-certificate-secret
```

* The entire `spec:` section should look like this:
```sh
spec:
  selector:
    istio: ingressgateway
  servers:
  - hosts:
    - '*'
    port:
      name: http
      number: 80
      protocol: HTTP
    tls:
      httpsRedirect: false
  - hosts:
    - "*"
    port:
      name: https
      number: 443
      protocol: HTTPS
    tls:
      mode: SIMPLE
      credentialName: kubeflow-certificate-secret
```
 
## UPDATE END


## UPDATE START

## Update the AWS Route53 DNS provider wih the Kubeflow IP address and desired Fully Qualified Domain Name of the Kubeflow UI

## Use a browser to connect, with HTTP (not HTTPS), to Kubeflow UI at the FQDN

* The screen should redirect to dex and offer a login prompt
* (Optional) Login with the credentials: 
`Email address`

```sh
user@example.com
```

`Password`
```sh
12341234
```


IMPORTANT: Proceed to the next section only after being able to connect to, and optionally, log into the Kubeflow UI



## Configure cert-manager to manage certificates from Let's Encrypt, using Route 53 DNS records

NOTE: cert-manager can manage certificates from any public DNS provider. See the cert-manager documentation at https://cert-manager.io/docs/configuration/acme/ for more information.

NOTE: An AWS user with appropriate IAM policies and API access keys is needed for cert-manager to access the Route53 DNS records. See the cert-manager documentation at https://cert-manager.io/docs/configuration/acme/dns01/route53/ for more information.

.Create a cert-manager Issuer for Let's Encrypt:
* Set these variables:
```sh
# aws_access_key_id and aws_secret_access_key for the configured AWS user:
export AWS_ACCESS_KEY_ID=""
export AWS_SECRET_ACCESS_KEY=""
export AWS_REGION="" # E.g. "us-west-2"
export DNSZONE="" # E.g. "suse.com"
export FQDN="" # E.g. "kubeflow.suse.com"
export EMAIL_ADDR="" # valid email address for the Let's Encrypt certificate

```

NOTE: When initially creating the cert-manager Issuer, ensure the `server: https://acme-staging-v02` line is uncommented and the `server: https://acme-v02` line is commented out. After verifying that the certicate can be issued correctly, we will reverse this to obtain the valid, production certificate.

* Create the cert-manager Issuer file:
```sh
cat <<EOF> letsencrypt-issuer.yaml
apiVersion: cert-manager.io/v1
kind: Issuer
metadata:
  name: letsencrypt-issuer
  namespace: istio-system
spec:
  acme:
    email: ${EMAIL_ADDR}
    server: https://acme-staging-v02.api.letsencrypt.org/directory # Use this line to test the process of issuing a certificate to avoid the Let's Encrypt production rate limits
#    server: https://acme-v02.api.letsencrypt.org/directory # Use this line after the certificate issues correctly
    privateKeySecretRef:
      name: letsencrypt-issuer-priv-key # K8s secret that will contain the private key for this, specific issuer
    solvers:
    - selector:
        dnsZones: 
          - "${DNSZONE}"
      dns01:
        route53:
          region: ${AWS_REGION}
          accessKeyID: ${AWS_ACCESS_KEY_ID}
          secretAccessKeySecretRef:
            name: route53-credentials-secret
            key: secret-access-key
EOF
```

IMPORTANT: Review the letsencrypt-issuer.yaml file for accuracy before continuing

* Verify the update to the file:
```sh
cat letsencrypt-issuer.yaml
```


* Create the `letsencrypt-issuer` resource:

```sh
kubectl apply -f letsencrypt-issuer.yaml
```

* Create the Kubernetes secret containing the aws_secret_access_key for the AWS user:

```sh
kubectl create -n istio-system secret generic route53-credentials-secret --from-literal=secret-access-key=${AWS_SECRET_ACCESS_KEY}`
```

* Verify the contents of the secret:
```sh
kubectl get -n istio-system secret route53-credentials-secret -o jsonpath={.data.secret-access-key} | base64 -d; echo ""
```

## Update OIDC to allow the Let's Encrypt DNS01 challenge:

* From inside the Kubeflow `manifests` directory (i.e. the base directory from the cloned https://github.com/kubeflow/manifests repository), update the oidc-authservice params.env file:

```sh
cp -p common/oidc-authservice/base/params.env /tmp/params.env.orig
sed -i 's/SKIP_AUTH_URI=\/dex/SKIP_AUTH_URI=\/dex \/.well-known/' common/oidc-authservice/base/params.env
sed -i 's/SKIP_AUTH_URLS=\/dex/SKIP_AUTH_URI=\/dex \/.well-known/' common/oidc-authservice/base/params.env
```

* Verify the file now contains `/dex /.well-known` on the SKIP_AUTH... line:
```sh
cat common/oidc-authservice/base/params.env
```

* Update the running oidc-authservice instance and return the file to its original state:
```sh
kustomize build common/oidc-authservice/base | kubectl apply -f -

sleep 5

cp -p /tmp/params.env.orig common/oidc-authservice/base/params.env
```

* Verify the hostname for the certificate resolves correctly:
```sh
getent hosts ${FQDN}
```

* Create the cert-manager Certificate resource file:
```sh
cat <<EOF> kubeflow-certificate.yaml 
apiVersion: cert-manager.io/v1
kind: Certificate
metadata:
  name: kubeflow-certificate
  namespace: istio-system
spec: 
  secretName: kubeflow-certificate-secret # Kubernetes secret that will contain the tls.key and tls.crt of the new cert
  commonName: ${FQDN}
  dnsNames:
    - ${FQDN}
  issuerRef:
    name: letsencrypt-issuer
    kind: Issuer
EOF
```

* Verify the Certificate resource file:
```sh
cat kubeflow-certificate.yaml
```

* Create the Certificate resource:
```sh
kubectl apply -f kubeflow-certificate.yaml
```

* Check the status of the certificate:
```sh
kubectl get -w -n istio-system certificate
```
** Use `Ctrl+c` to exit the kubectl watch (-w) command


NOTE: The certificate can take up to three minutes to be issued, as indicated by the `READY` status becoming `True`


* If needed, check the progress of the certificate:
```sh
kubectl describe -n istio-system certificate kubeflow-certificate
```

IMPORTANT: If the certificate seems to be taking a long time to be issued, review the cert-manager logs for clues. Common errors are related to DNS resolution, credentials, and IAM policies. Keep checking back for the status of the certificate since it will likely keep working in the background. 

* If needed, review the cert-manager logs:
```sh
kubectl logs -n cert-manager -l app=cert-manager
```

IMPORTANT: Proceed to the next section only after the certificate shows a `READY` status of `True` 

## Update the gateway to use TLS with the cert

## Use a browser to connect with HTTPS to Kubeflow UI 

NOTE: Since the certificate was issued by the Let's Encrypt *Staging* servers, it will cause an error in the browser that it is untrusted. 

* Click the lock icon in the browser's URL pane, then continue selecting appropriate options until you are able to review the connection certificate. It should say that the certificate was issued by Let's Encrypt (Staging)

## Update the configuration to use a production Let's Encrypt certificate

* Update the letsencrypt-issuer.yaml file to comment out the `server: https://acme-staging-v02` line and uncomment the `server: https://acme-v02` line:


* Update the `letsencrypt-issuer` resource:

```sh
kubectl apply -f letsencrypt-issuer.yaml
```

* Remove the certificatate and its associated secret:

* Recreate the certificate:

* Reload the browser window to verify the publicly signed certificate



* After ready becomes true (about 3 minutes):  
** change issuer from staging to prod 
** delete the secret containing the cert and key
** recreate the cert


## UPDATE END

////
NOTE: MetalLB is configured to provide a Virtual IP address to `istio-ingressgateway`. However, as described at this page (https://github.com/kubeflow/manifests#nodeport--loadbalancer--ingress), TLS must be configured first. After TLS has been configured, the `istio-ingressgateway` service in the `istio-system` namespace can be edited to change the service type from `ClusterIP` to `LoadBalancer`.
////

