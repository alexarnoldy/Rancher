### Current state of this doc (03/30/23): Constant flux


IMPORTANT: This guide assumes Rancher Management Server is installed and configured with Harvester under Rancher management.


# Overall steps to complete this task

. Deploy an RKE2 cluster on Harvester
. Deploy the NFS server
. Create the NFS client Container Storage Interface (CSI) on the RKE2 cluster
. Deploy the MetalLB load balancer
. Verify MetalLB and the Longhorn CSI are working correctly
. Deploy Istio
. Prepare the Kubeflow deployment files
. Deploy Kubeflow
. Connect to the Kubeflow UI

# Steps that are currently in moderate flux

.Prepare the RKE2 Kubernetes cluster
* For these tests, we used Harvester, managed through Rancher, to create a five node RKE2 cluster
* A project in Harvester named kubeflow-on-harvester contains a namespace named kubeflow-cluster

.Create a cluster through the Rancher UI

NOTE: The resource allocations used here were for basic testing purposes. It is likely the more CPU and RAM would be required for the workload-plane VMs to support a useful Kubeflow workload.

* The cluster name set to: 
```sh
kubeflow-on-harvester
```
* Three instances set with the roles of control-plane and etcd pool
** Configure resources of 4 vCPU, 8GB RAM, 40GB boot drive
** Two VMs make up the workload-plane pool:
*** 8 vCPU, 16GB RAM, 40GB boot drive
* All nodes should be in the same Harvester namespace
* The operating systems for all nodes is the SUSE Linux Enterprise 15 SP4 minimal QCOW2 image with cloud-init enabled (previously known as the OpenStack image)
* All nodes are connected to a single VLAN with DHCP, DNS, and routing to the Internet
* The SSH user for the O/S image is 
```sh
sles
```


* The following User Data cloud-config (under `Show Advanced`) was applied to all nodes during RKE2 cluster creation:
```sh
### cloud-init
#cloud-config
chpasswd:
  list: |
    root:SUSE
    sles:SUSE
  expire: false
ssh_authorized_keys:
  - >-
    <REPLACE WITH SSH PUBLIC KEY>
runcmd:
  - SUSEConnect --url <REPLACE WITH RMT SERVER ADDRESS>
  - zypper -n in -t pattern apparmor
  - zypper -n up
  - zypper in --force-resolution --no-confirm --force kernel-default
  - zypper rm kernel-default-base
```

* The Kubernetes `Cluster Configuration` is as follows:
** Kubernetes version v1.24.9+rke2r2 (currently depricated, but needed for Harvester Cloud Provider support)
** Enable the Harvester Cloud Provider CSI driver
** `Container Network` Interface is Calico
** Set the `Default Security Pod Policy` to `unrestricted`
** Disable the `Nginx Ingress` controller under `System Services`
** Apply a cluster label of `app=kubeflow` 

.Verify and reboot the RKE2 nodes
* Verify that the `kernel-default` kernel has been installed and `kernel-default-base` kernel has been removed: `sudo zypper se kernel-default`
** If needed, remove the `kernel-default-base` kernel with: `sudo zypper rm kernel-default-base`
* Verify that all operating system software has been patched to the latest update: `sudo zypper up`
* Will need to reboot each node, in turn to enable the kernel-default kernel

////
.Prepare NFS server (used master node VM in these tests)
* Basic NFS deployment
* Can use `sudo showmount -e` on the server to verify its serving and `sudo showmount -e <NFS server IP>` from the worker nodes to verify they have everything needed to mount
* Using Kubeflow version 1.6
////


## After the RKE2 cluster has been created, gather the KUBECONFIG data from the Rancher Management server and provide it to a workstation with kubectl and helm installed

## Deploy MetalLB

* Use these instructions to deploy MetalLB on the RKE2 cluster: https://gist.github.com/alexarnoldy/24dd06d8c4291d04c5d7065b520bcb15

NOTE: The instructions in the link above include testing the MetalLB service after deployment. This can be omitted as both MetalLB and the Longhorn CSI will be tested in the next step.

## Test MetalLB and the Longhorn CSI

* Set this variable with the target namespace: `NAMESPACE="kubeflow"`

* Create the namespace: `kubectl create namespace ${NAMESPACE}`

* Create the manifest for an nginx pod, PVC, and load balancer service:

----
cat <<EOF> nginx-metallb-test.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx
  namespace: ${NAMESPACE}
spec:
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1
        ports:
        - name: http
          containerPort: 80
        volumeMounts:
        - mountPath: /mnt/test-vol
          name: test-vol
      volumes:
      - name: test-vol
        persistentVolumeClaim:
          claimName: nginx-pvc


---
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: nginx-pvc
  namespace: ${NAMESPACE}
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi


---
apiVersion: v1
kind: Service
metadata:
  name: nginx
  namespace: ${NAMESPACE}
spec:
  ports:
  - name: http
    port: 8080
    protocol: TCP
    targetPort: 80
  selector:
    app: nginx
  type: LoadBalancer
EOF
----

* Create the pod, service, and the PVC: `kubectl apply -f nginx-metallb-test.yaml`
* Verify the pod is "Running", the persistentvolumeclaim is "Bound", and the service has an "EXTERNAL-IP": `kubectl get pod,pvc,svc -n ${NAMESPACE}`
* Verify that the service is reachable through the load balancer IP address from outside the cluster:

----
IPAddr=$(kubectl get svc -n ${NAMESPACE} | grep -w nginx | awk '{print$4":"$5}' | awk -F: '{print$1":"$2}')
curl http://${IPAddr} | grep "Thank you"
----

** An HTML encoded output should display the phrase "Thank you for using nginx."

* Verify that the volume is mounted in the test pod: 

----
TEST_POD=$(kubectl get pods -n ${NAMESPACE} | awk '/nginx/ {print$1}')
kubectl exec -it ${TEST_POD} -n ${NAMESPACE} -- mount | grep test-vol
----
** The output should show that the volume is mounted at the location `/mnt/test-vol` 

* When finished with testing, delete the pod and service: 
```sh
kubectl delete -f nginx-metallb-test.yaml
```


////

*****Omitted in favor of Longhorn*****
.Deploy nfs-client-provisioner from Helm catalog
* From top menu bar, point to "Global" or the cluster name, then point to the cluster name just below it, then select "Default" project
* From top menu bar, select "Apps", then select "Launch"
* Search for "nfs-client-provisioner", then select it
** Under "Answers", paste the following into the first "Variable" answer box:
----
nfs.server=IPAddress
nfs.path=FullyQualifiedPath
storageClass.name=nfs
storageClass.defaultClass=true
----
*** Replace "IPAddress" with the hostname or IP address of the NFS server (RKE master node in these tests)
*** Replace "FullyQualifiedPath" with the fully qualified path of the NFS share
* Select "Launch" at the bottom of the page

////



===== Enable Istio

NOTE: This guide assumes Istio was not installed when the RKE2 cluster was instantiated. 

NOTE: Installing Istio through the Rancher App Catalog requires that Rancher Monitoring be installed first.

.Install Rancher Montoring via the Rancher Manager UI
* From the "Cluster Managment" GLOBAL APP, select "Explore" on the target RKE2 cluster
* Select "Cluster Tools", then click on "Install" for `Monitoring`
* Select the appropriate version of the Rancher chart (the latest was used for this guide)
* Install into the "System" project
* Click `Next`
* Accept the default settings on the next page
* Click `Install`
* The installation will open a terminal window in the bottom section of the Rancher Manager UI
* Keep that terminal window open until it completes with an output that includes: `SUCCESS: helm upgrade --install=true ...`

.Install Istio via the Rancher Manager UI
* From the "Cluster Managment" GLOBAL APP, select "Explore" on the target RKE2 cluster
* Select "Cluster Tools", then click on "Install" for `Istio`
* Select the appropriate version of the Rancher chart (the latest was used for this guide)
* Install into the "System" project
* Select `Customize Helm options before install`
* Click `Next`
* Accept the default Components on the next page
* Click `Edit YAML`
** In the YAML file, change ingressGateways.type to `LoadBalancer`
* Click `Install`
* The installation will open a terminal window in the bottom section of the Rancher Manager UI
* Keep that terminal window open until it completes with an output that includes: `SUCCESS: helm install --namespace=istio-system ...`

////
** (Optionally) Provide an IP address that is assigned to MetalLB but not in use
////

////
******Don't think this is a very good test at this point******

* Use kubectl on the workstation CLI to Validate the istio-ingressgateway has received an IP address: `kubectl get svc -A | egrep --color 'EXTERNAL-IP|LoadBalancer'`
** (Optionally) Validate an external connection to an internal Istio service: 
*** Use the curl command to connect to a few of the *PORT(S)* listed for the istio-ingressgateway, i.e. `

----
kubectl get svc -n istio-system istio-ingressgateway | awk '{print$5}'
curl http://{$IPADDR}:15020
----

*** At least one of the ports should return "404 page not found"
////

====Install Kubeflow

NOTE: The instructions for this secion are taken directly from: `https://github.com/kubeflow/manifests#installation`

# Steps that are currently in extreme flux
.Prepare the Kubeflow deployment files on the workstation
* Install the kfctl utility and place it in /usr/local/bin:
----
wget https://github.com/kubeflow/kfctl/releases/download/v1.1.0/kfctl_v1.1.0-0-g9a3621e_linux.tar.gz
tar xvfz kfctl_v1.1.0-0-g9a3621e_linux.tar.gz 
sudo mv kfctl /usr/local/bin
kfctl version
----

* Configure the following variables (adjust as needed)
----
export KF_NAME=kubeflow-deployment
export BASE_DIR=${HOME}
export KF_DIR=${BASE_DIR}/${KF_NAME}
export CONFIG_URI="${KF_DIR}/kfctl_k8s_istio.v1.0.2.yaml"
----

* Create and enter the ~/kubeflow-deployment directory: `mkdir -p ${KF_DIR} && cd ${KF_DIR}`
* Download the kfctl.yaml config file: `wget https://raw.githubusercontent.com/kubeflow/manifests/v1.0-branch/kfdef/kfctl_k8s_istio.v1.0.2.yaml`
* The following section of the kfctl_k8s_istio.v1.0.2.yaml manifest will install and enable Istio
** If Istio is installed and enabled, comment out the following lines, near the top of the kfctl_k8s_istio.v1.0.2.yaml file
----
  - kustomizeConfig:
      parameters:
      - name: namespace
        value: istio-system
      repoRef:
        name: manifests
        path: istio/istio-crds
    name: istio-crds
  - kustomizeConfig:
      parameters:
      - name: namespace
        value: istio-system
      repoRef:
        name: manifests
        path: istio/istio-install
    name: istio-install
----

* Download the Kubeflow build files: `kfctl build -V -f ${CONFIG_URI}`

===== Create a new pod security policy to support Kubeflow

NOTE: This section assumes there is not an adequate pod security policy available in the cluster and/or the user needs help in configuring one. The PSP created here is the most privileged and the least secure PSP possible. Use at your own risk.

* Create the PSP manifest file:
----
cat <<EOF> kubeflow-privileged-psp.yaml
apiVersion: policy/v1beta1
kind: PodSecurityPolicy
metadata:
  annotations:
    seccomp.security.alpha.kubernetes.io/allowedProfileNames: '*'
  name: kubeflow-privileged-psp
spec:
  allowPrivilegeEscalation: true
  allowedCapabilities:
  - '*'
  fsGroup:
    rule: RunAsAny
  hostIPC: true
  hostNetwork: true
  hostPID: true
  hostPorts:
  - max: 65535
    min: 0
  privileged: true
  runAsUser:
    rule: RunAsAny
  seLinux:
    rule: RunAsAny
  supplementalGroups:
    rule: RunAsAny
  volumes:
  - '*'
EOF
----

* Create the new PSP: `kubectl apply -f kubeflow-privileged-psp.yaml`

.Update the kubeflow-edit and kubeflow-admin cluster roles in the cluster-roles.yaml manifest file to use the new PSP

////
* Create an aggregated cluster role file that will update the cluster roles during installation:

----
cat <<EOF> psp-update-kubeflow-edit-kubeflow-admin.yaml

---

apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: psp-update-kubeflow-edit-kubeflow-admin
  labels:
    rbac.authorization.kubeflow.org/aggregate-to-kubeflow-admin: "true"
    rbac.authorization.kubeflow.org/aggregate-to-kubeflow-edit: "true"
rules:
- apiGroups:
  - policy
  resourceNames:
  - kubeflow-privileged-psp
  resources:
  - podsecuritypolicies
  verbs:
  - use
EOF
----
* Save a copy of the kustomize/kubeflow-roles/base/cluster-roles.yaml file: `cp -p kustomize/kubeflow-roles/base/cluster-roles.yaml /tmp/`
* Append the psp-update-kubeflow-edit-kubeflow-admin.yaml file to the end of the kustomize/kubeflow-roles/base/cluster-roles.yaml file: `cat psp-update-kubeflow-edit-kubeflow-admin.yaml >> kustomize/kubeflow-roles/base/cluster-roles.yaml`
* Verify the changes: `diff kustomize/kubeflow-roles/base/cluster-roles.yaml /tmp/cluster-roles.yaml`
////

* Save a copy of the kustomize/kubeflow-roles/base/cluster-roles.yaml file: `cp -p kustomize/kubeflow-roles/base/cluster-roles.yaml /tmp/`
* Edit the kustomize/kubeflow-roles/base/cluster-roles.yaml file
** Search for kubeflow-kubernetes-admin 

NOTE: Ensure the "resourceNames" refers to the correct PSP to be used.

** Insert the following lines under the "rules:" section of the kubeflow-kubernetes-admin ClusterRole:
----
- apiGroups:
  - policy
  resourceNames:
  - kubeflow-privileged-psp
  resources:
  - podsecuritypolicies
  verbs:
  - use
----
** Search for kubeflow-kubernetes-edit 
** Insert the same lines under the "rules:" section of the kubeflow-kubernetes-edit ClusterRole
** Save and close the file
* Verify that only the intended changes were made to the file: `diff kustomize/kubeflow-roles/base/cluster-roles.yaml /tmp/cluster-roles.yaml`

.Deploy Kubeflow

* Ensure these variables are still set correctly:
----
echo ${KF_NAME}
echo ${BASE_DIR}
echo ${KF_DIR}
echo ${CONFIG_URI}
----

* Start the deployment: `kfctl apply -V -f ${CONFIG_URI}`

* From another terminal, use the following command to monitor the kubeflow deployment: `watch 'kubectl get pods -A | egrep -v "Completed|Running"'`

** Over time, the number of pods that are in a state of `ContainerCreating` should decrease.

.Connect to the Kubeflow web UI

* Use the follow command to find the load balancer IP address (under EXTERNAL-IP) to connect to the Kubeflow UI: `kubectl get svc -n istio-system | egrep 'EXTERNAL-IP|LoadBalancer'`
* Connect to the Kubeflow UI through a web browser pointed to the external IP address on port 80

NOTE: During the first, successful test it took several hours for all of the deployments to deploy their pods. I really thought it was one of the worst failures to date, but many hours later I discovered virtually everything deployed correctly.

IMPORTANT: On every attempt at least one pod had not deployed correctly. If there are only a few, or less, Navigate to "Workloads" in the "Default Project" and delete one, wait for it to re-deploy correctly, then move on to the next one. It can take several minutes for each pod to finish re-deploying correctly.

CAUTION: I am still experiencing a situation where the kubeflow-edit cluster role loses the entries for the pod security policy that is assigned to it in the ~/kubeflow-deployment/kustomize/kubeflow-edit.yaml file. The result is that Jupyter Notebook can't deploy servers due to lack of a compatible PSP. 
